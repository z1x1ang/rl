<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Reinforcement Learning</title>
		<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/reveal.min.css'>
		<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/css/theme/white.min.css'>
		<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/lib/css/zenburn.min.css'>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/5.0.2/es5/tex-mml-chtml.js"></script>
		<link rel="stylesheet" type="text/css" href="style.css"></link>
<style>
	.reveal{
transform: scale(0.92);
	}
</style>
	</head>

	<body>

		<div class="reveal">
	<!-- Any section element inside of this container is displayed as a slide -->
	<div class="slides" >
		<section>
			<h2>Reinforcement Learning and Video Games</h2>
			<h3>强化学习技术在游戏场景下的实践与应用</h3>
			<p>
				<small>指导老师:马碧阳&nbsp;&nbsp;&nbsp;&nbsp;<i>Created by <a href="https://ar5iv.labs.arxiv.org/html/1909.04751">Yue Zheng</a> from <a
						href="https://www.sheffield.ac.uk/">University of Sheffield</a></i>&nbsp;&nbsp;&nbsp;&nbsp;汇报人:田子祥</small><br>
			</p>
			<img src="img/runner.gif" width="60%" style="margin-top: -5px;" >
			<aside class="notes">
				大家好，今天我汇报的论文是<<强化学习技术在游戏场景下的实践与应用>>,大家平常接触最多的是监督学习...无...以及半...,它们都是从带有标签(引导词)或者...去学习训练,强化学习...,它更多的是与环境不断地交互训练,最终找到最优策略，获得最大化的长期奖励。它的应用也非常多
				比如自动驾驶领域、机器人控制领域、游戏领域、等等。今天要讲的就是在....,...观察...,...尽可能多的奖励...,今天咱们的目的就是使用几种强化学习算法去训练这个智能体，让它尽可能跑的更远，得到更多的分数。
			</aside>
		</section>
		<div style="display: flex; align-items: flex-start;">
			<img src="img/logo1.png" width="43">
			<img class="r-frame" src="img/logo2.png" width="51" style="margin-left: 0px;margin-top: -3px;">
		</div>
		<section>
			<h2>概述</h2>
			<p>
				通过结合强化学习和深度学习的方法，智能体在游戏领域已经超越了人类专家的水平。这一点,通过DeepMind在围棋比赛中的获胜以及智能体在雅达利游戏中表现得到了证明。<span class="simple-highlight">深度学习技术解决了多年来一直阻碍强化学习发展的高维输入问题。</span><mark>在本研究中,我们结合了这两种技术,创建了几种不同算法的智能体,并让这些智能体学会了玩T-rex Runner游戏。</mark>我们实现了深度Q网络算法及其三种改进类型,以训练这些智能体。虽然其中一些结果并不尽如人意，但其他一些结果却优于人类玩家。此外,批量归一化是一种解决深度神经网络内部协变量偏移问题的方法，本研究也证明了它对强化学习的积极影响。
			</p>
			<aside class="notes">
				先看下概述部分讲的主要是强化学习在游戏中的表现，比如打败....,在这里，结合深度学习技术，...特征提取，...游戏开发好后，不是...,游戏封装度非常高...也就是提取一帧一帧的游戏画面...一个一个像素点...,rgb三层,
	            传统强化学习方法：传统强化学习通常依赖于预先定义好的状态空间和可能的动作集。在处理视频数据时，这些方法面临着重大挑战，因为视频数据通常非常高维（每帧图像由大量像素组成），而且状态空间难以预先定义。因此，传统强化学习方法在直接从原始视频数据中学习时通常效果不佳。

深度强化学习：深度强化学习通过结合深度学习（特别是卷积神经网络，CNN）与强化学习，能够直接从高维输入（如视频）中学习。深度学习部分负责从原始像素数据中自动提取特征和表示状态，而强化学习部分则使用这些特征来学习如何在环境中做出决策。这种方法使得智能体可以直接从视频流中学习，而不需要人工设计状态空间。

例如，在DeepMind开发的深度强化学习系统中，智能体能够直接从原始的游戏画面像素数据学习玩雅达利游戏。这表明深度强化学习技术可以有效地从视频数据中学习，而传统的强化学习方法在这方面则受到限制。
批量归一化的作用：批量归一化是一种常用于深度学习的技术，用于加快训练速度、提高稳定性和性能。在强化学习中，批量归一化可以帮助智能体更快地适应环境，特别是在处理那些具有大量输入特征的复杂环境时。
批量归一化（Batch Normalization，简称BN）是一种在深度学习中常用的技术，主要用于改善网络训练过程。它通过调整每层输入的分布来加速训练，提高稳定性，并有助于减少对其他参数（如学习率）的敏感度。下面是对批量归一化的详细解释和一个类比：

### 批量归一化的工作原理：

1. **标准化处理**：在训练过程中，批量归一化对每个小批量（batch）数据的每个特征分别进行标准化处理。具体来说，它会计算该批量中每个特征的均值和标准差，并使用这些值来将特征值转换为均值为0、标准差为1的分布。

2. **缩放和偏移**：标准化后，批量归一化引入了两个可学习参数，分别是缩放（scale）和偏移（shift）参数。这些参数用于从标准化的分布中恢复出适合网络学习的特定分布。这一步骤确保了批量归一化既可以进行必要的标准化，又保持了网络学习特定分布的能力。

3. **减少内部协变量偏移**：批量归一化通过这种方式减少了所谓的内部协变量偏移（Internal Covariate Shift），即网络中间层输入分布的变化。这使得网络训练过程更加稳定，同时允许使用更高的学习率，从而加速训练过程。

### 类比解释：

将批量归一化比作是一种“气候控制”系统。想象一下，如果一个工厂的机器需要在特定的温度和湿度下运行才能最高效，但外界的气候却在不断变化（太热、太冷、太湿或太干）。这些变化可能会影响机器的运行效率。

在这个比喻中，机器就像是深度学习模型的各层，而外界气候的变化就像是训练过程中数据分布的变化。批量归一化就像是一个气候控制系统，它不断调节输入数据的“气候”（即均值和方差），确保机器（即网络层）总是在最佳的“气候”条件下工作。这样，无论外界条件如何变化，机器都能保持稳定的运行效率。

总结来说，批量归一化通过标准化训练数据的方式，帮助深度学习模型更快、更稳定地学习，类似于为模型提供了一个稳定的“工作环境”。这样，模型就可以更专注于学习特定任务，而不是应对输入数据分布的变化。

			</aside>
		</section>
		<section>
			<h2>目录</h2>
			<ol>
				<li>项目目的及介绍</li>
				<li>预备知识</li>
				<li>实现方法</li>
				<li>实验结果及讨论</li>
				<li>结论及未来展望</li>
			</ol>
		</section>
		<!-- Example of nested vertical slides -->
		<section>
			<section>
				<h2>1.项目目的及介绍</h2>
				<p></p>
				<p>背景、目的、概述</p>
				<br>
				<a href="#" class="navigate-down">
					<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png"
						alt="Down arrow">
				</a>
			</section>
			  
				<h2>游戏介绍</h2>
				<p>T-rex Runner是一款来自Google Chrome离线模式的恐龙游戏。玩家的目标是躲避所有障碍并获得更高的分数,直到达到极限。</p>
				
			</section>
			<section>
				<h2>项目目的</h2>
				<p>该项目的目的是创建一个使用不同算法(<span class="simple-highlight">DQN,double DQN,DQN with prioritized experience replay,dueling DQN</span>)的智能体来玩T-rex Runner并比较它们的性能。
					<ul>
						<li>创建智能体来玩 T-rex Runner</li>
						<li>比较不同强化学习算法之间的差异</li>
						<li>研究<a>批量归一化</a>在强化学习中的作用</li>
					</ul>
				</p>
				<aside class="notes">
					DQN（深度Q网络）：

中文名：深度Q网络。
特点：结合了经典的Q学习算法和深度学习。使用卷积神经网络来估计Q值（即某状态下特定动作的价值）。DQN能够处理高维的输入空间（如像素级的图像），适用于复杂的决策任务。
Double DQN（双重DQN）：

中文名：双重DQN。
特点：为了解决DQN中的过估计（overestimation）问题。在这种方法中，选择动作和评估动作的Q值是通过两个不同的网络完成的，这有助于减少估计Q值时的偏差。
DQN with Prioritized Experience Replay（具有优先级经验回放的DQN）：

中文名：具有优先级经验回放的DQN。
特点：改进了DQN的经验回放机制，不是随机地从经验池中抽取样本，而是根据每个经验的优先级（通常与该经验的TD误差相关）来选择。这种方法使得模型更加高效地学习重要的经验。
Dueling DQN（对决DQN）：

中文名：对决DQN。
特点：在网络架构上的创新。它将Q值的估计分为两部分：一部分估计状态值（一个状态的绝对价值），另一部分估计动作优势值（相对于其他动作的价值）。这种分解方式有助于提升对状态价值和动作价值的估计精度。
				</aside>
			</section>


			<!-- <section>
				<h2>Basement Level 2</h2>
				<p>That's it, time to go back up.</p>
				<br>
				<a href="#/2">
					<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Up arrow"
						style="transform: rotate(180deg); -webkit-transform: rotate(180deg);">
				</a>
			</section> -->
		</section>
		<section>
			<section>
				<h2>2.T-rex runner所需知识</h2>
				<p></p>
				<p>深度学习、神经网络、激活函数、反向传播算法、卷积神经网络、批量归一化</p>
				<p>强化学习、马尔可夫决策过程、贝尔曼方程、探索和利用、TDL时序差分学习、Deep Q网络</p>
				<br>
				<a href="#" class="navigate-down">
					<img width="108" height="130" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png"
						alt="Down arrow">
				</a>
			</section>
			<section>
				<aside class="notes">
					在使用卷积神经网络（CNN）训练智能体玩T-Rex Runner（即Chrome浏览器中的离线恐龙游戏）时，CNN主要从图像数据中提取以下特征：

1. **物体形状和轮廓**：CNN能够识别游戏中不同物体（如障碍物和T-Rex恐龙）的形状和轮廓。这对于区分不同类型的障碍物（如仙人掌和飞鸟）尤其重要。

2. **物体位置和相对距离**：智能体需要了解障碍物的位置，特别是它们相对于T-Rex的距离。这有助于决定何时跳跃或下蹲。

3. **物体运动**：捕捉物体的运动（如飞鸟的飞行轨迹）对于预测未来状态和做出及时反应至关重要。

4. **游戏速度**：随着游戏进程，速度会逐渐提升。CNN可以从游戏环境中提取速度相关的特征，帮助智能体适应更快的游戏节奏。

5. **背景变化**：虽然T-Rex Runner的背景较为简单，但CNN仍可捕捉到背景的微小变化，这可能对判断游戏状态和进程有所帮助。

CNN在这个过程中通过其多个卷积层自动学习和提取这些特征。初始层通常捕捉更简单和通用的特征（如边缘和角点），而更深层则学习更复杂和抽象的特征（如特定物体的形状）。这些特征的组合使得智能体能够理解和有效地与游戏环境互动。
				</aside>
				<h2>深度学习</h2>
				<p>深度学习是一类基于人工神经网络（ANN）的机器学习模型。近年来广泛应用的深度学习模型有两种。递归神经网络就是其中之一，它在自然语言处理中展示了它的强大功能。另一个在深度强化学习中起着重要作用的模型<span class="simple-highlight">称为卷积神经网络 （CNN）。它是应对计算机视觉问题（如物体检测和图像分类）最有效的模型之一。</span>CNN在本项目中主要用于从图像数据中提取特征。</p>
			</section>
			<section>
				<h2>深度神经网络和激活函数</h2>
				<p>神经网络或多层感知器主要由三个核心部分构成：输入层、隐藏层和输出层。每个层中的单元被称为神经元。输入数据首先经过输入层，然后传入隐藏层，在这里通过权重进行线性变换。接着，激活函数对这些线性变换的结果赋予非线性特性，并将其传递到输出层。</p>
				<img width="650px" src="https://ar5iv.labs.arxiv.org/html/1909.04751/assets/figures/dnn.png">
			<aside class="notes">
			

想象一下，神经网络就像是一个处理信息的工厂，这个工厂有三个主要的工作区域：输入层、隐藏层和输出层。

1. **输入层**：这就像是工厂的原材料接收区。在这里，我们得到原始数据，可以把它看作是原材料，比如一批苹果。

2. **隐藏层**：这是工厂中的加工车间。原材料（数据）在这里被加工。首先，它们通过一些机器（这里指“权重”）进行初步加工，这就像是把苹果切片。这个过程是线性的，就像是简单的切割。但仅仅切割是不够的，我们需要给这些苹果切片更多的风味和特色，这就是“激活函数”的作用。它就像是在苹果切片上加上不同的调料，使它们具有独特的风味。

3. **输出层**：最后，加工后的产品（即经过处理的数据）被送到输出区，这就是我们的最终产品，比如成品的苹果派。输出层决定了这个派最终的形状和口味，就像是我们的神经网络最终得出的结果。

整个过程，从原材料到最终产品，就像是数据在神经网络中的流动和转化过程。每一步都是必要的，每一步都为最终的结果添加了独特的价值。在处理T-Rex Runner游戏画面时，神经网络（尤其是卷积神经网络，CNN）的作用可以通过类似的比喻来说明：

1. **输入层**：在这个阶段，游戏的每一帧画面就像是原材料，被送入神经网络的“工厂”。每一帧画面是由大量像素组成的，这些像素包含了关于游戏当前状态的所有可视信息，比如T-Rex的位置、障碍物的位置和类型等。

2. **隐藏层**：这些画面像素随后进入隐藏层，这里可以视为加工车间。在隐藏层中，卷积神经网络通过其卷积层开始“加工”这些像素。初级卷积层可能会识别出简单的图像特征，如边缘和角点。随着数据通过更深层的网络，神经网络开始识别更复杂的特征，比如障碍物的形状或T-Rex的动作。这个过程可以比作是先将苹果切片（识别简单特征），然后根据苹果切片制作出复杂的苹果派（识别复杂特征）。

3. **激活函数**：在这一过程中，激活函数对原始的线性输出赋予非线性特性，使得网络能够学习和模拟更复杂的模式和决策。这就像是给苹果切片加上各种调料，使之变得更有特色。

4. **输出层**：最后，在输出层，神经网络产生一个决策或预测。在T-Rex Runner游戏中，这个决策可能是让T-Rex跳跃、下蹲或者继续前进。输出层决定了这个动作，基于之前所有加工过程的结果。

总的来说，在T-Rex Runner游戏中，神经网络的作用是从原始的游戏画面中提取关键特征，通过一系列复杂的加工过程，最终生成决策动作。这个过程使得神经网络能够理解游戏环境，并学会如何在游戏中做出有效的反应。

			</aside>
			</section>
			<section>
				<h2>批量归一化</h2>
				<p>
					随着神经网络深度的增加，训练时间变得更长。其原因之一是，在更新权重时，每层的输入分布会发生变化，这被称为内部协变量偏移（Internal Covariate Shift）。2015年，Ioffe提出了批量归一化（Batch Normalization，BN），使每层的分布更稳定，并实现了更短的训练时间 [15]。在每个神经元中，输入可以通过方程进行归一化。				</p>
		<aside class="notes">
			在T-Rex Runner游戏的神经网络训练过程中，批量归一化（Batch Normalization）起着非常关键的作用。让我们通过之前的比喻来进一步理解批量归一化的作用：

1. **保持稳定的加工条件**：回到我们的工厂比喻，批量归一化就像是确保工厂内部环境（比如温度和湿度）保持一致的系统。在神经网络的背景下，这意味着它确保数据在流经神经网络的各个层时，其分布保持稳定。这对于有效地训练网络是非常重要的。

2. **提高训练效率**：在没有批量归一化的情况下，神经网络的每一层都需要适应前一层输出数据分布的变化，这会使得训练过程变得复杂且低效。批量归一化通过规范化这些数据，使得每一层都在相对稳定的数据分布下工作，从而加速训练过程。

3. **减少内部协变量偏移**：在神经网络训练过程中，随着前面层参数的更新，后面层的输入分布会不断变化，这被称为内部协变量偏移。批量归一化通过规范化层输入来减少这种偏移，使得网络训练更加稳定。

4. **允许使用更高的学习率**：由于批量归一化帮助减少了训练过程中的不稳定性，它允许使用更高的学习率，这进一步加速了网络的训练。

在T-Rex Runner的上下文中，批量归一化使得神经网络能够更有效地从游戏画面中学习，提高了学习过程的稳定性和效率。这意味着智能体可以在较短的时间内更好地学习如何避开障碍物，提高游戏性能。 
		</aside>
				</section>
			<section>
				<h2>强化学习</h2>
				<p>强化学习(RL)是一类机器学习，旨在在做出决策时获得最大的奖励信号。强化学习的基本组成部分是智能体和环境。如图 2.7 所示，智能体将在每次操作后收到来自环境的反馈，包括观察和奖励。为了产生更好的政策，它将不断与环境互动，并逐步提高其决策能力，直到政策趋同。</p>
				<img src="img/RL.PNG"  width="350px" alt="强化学习图">
			</section>
			<section>
				<h2>马尔可夫决策过程</h2>
				<p>马尔可夫属性:给定一个状态在时间在有限序列中.此序列具有马尔可夫性质，当且仅当\[P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...S_t]\]马尔可夫决策过程(MDP)是一个具有马尔科夫属性、值和决策的随机过程。</p>
				<p>状态转移概率\(P^a_{ss'}=P[S_{t+1}=s'|S_t=s,A_t=a]\)</p>
				<p>奖励\(R^a_{s}=E[R_{t+1}|S_t=s,A_t=a]\)</p>
				<p>策略\(\pi(a|s) = P(A_t=a|S_t=s)\)</p>
			</section>
			<section>
				<h2>回报\(G_t\)</h2>
				<p>\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]</p>
				<img src="img/MDP.PNG" alt="mdp.png">
			</section>
			<section>
				<h2>值函数</h2>
				<p>状态值函数\(V_{\pi}(s) = E_{\pi}[G_t | S_t = s]\)</p>
				<p>动作值函数\(Q_{\pi}(s,a) = E_{\pi}[G_t | S_t = s, A_t = a]\)</p>
			    <p>\[V_{\pi}(s)=\sum_{a \in A} {\pi}(a|s)Q_{\pi}(s,a)\]</p>
			</section>
			<section>
				<h2>贝尔曼期望方程</h2>
				<p>\(V_{\pi}(s)=E_{\pi}[R_{t+1}+\gamma V(S_{t+1})|S_t=s]\)</p>
				<p>\(Q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma Q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]\)</p>
				<a>公式证明</a>
			</section>
			<section>
				<h2>最优函数</h2>
				<p>\(V^*(s)=\max_{\pi}V_{\pi}(s)\) &nbsp;&nbsp; \(Q^*(s,a)=\max_{\pi}(s,a)\)</p>
				<p>贝尔曼最优方程:\(V^*(s)=\max_aR_s^a+\gamma \sum_{s'\in S}P^a_{ss'}V^*(s')\)</p>
				<p>最优动作:\(Q^*(s,a)=R^a_s+\gamma \sum_{s'\in S}P^a_{ss'}\max_{a'}Q^*(s',a')\) <a>[公式证明]</a></p>
				<p>\[Q_{\pi}(s,a)=R^a_s+\gamma \sum_{s' \in S}P^a_{ss'} \sum_{a' \in A} {\pi}(a'|s')Q_{\pi}(a'|s')\]</p>
				<p>\({\pi}^*(a|s) = 
					\begin{cases} 
					1 & a=argmax_{a \in A}Q^*(s,a) \\
					0 & 其他 
					\end{cases}\)
				</p>
			</section>
			<section>
				<h2>探索和利用<br>Exploitation vs Exploration</h2>
				<p>如果智能体对环境有完整的了解，换句话说，转移概率\(P^a_{ss'}\)可以计算给定状态s和行动a, 可以通过迭代方法求解，并具有合适的\(\gamma\)
					.但是，这种方法无法处理未知环境，因为必须收集大量信息才能进行估计\(P^a_{ss'}\)在收敛动作值函数之前。如果在环境被充分探索之前，功能趋于稳定，模型的性能将远远不能令人满意，特别是在高作用空间情况下。</p>
				<p>
					\({\pi}(a|s) = 
					\begin{cases} 
					\frac{\varepsilon}{m}+1-{\varepsilon} & a^*=argmax_{a \in A}Q(s,a) \\
					\frac{\varepsilon}{m} & 其他 
					\end{cases}\)
				</p>
			</section>
			<section>
				<h2>TDL时序差分</h2>
				<p>蒙特卡罗方法是一种不需要知道环境的模型 (model) 的方法，也就是说，智能体不需要知道在每个状态 (state) 下采取每个动作后会转移到哪个状态，以及得到多少回报 (reward)。智能体只需要从实际或模拟的环境中采样数据，也就是一系列的状态、动作和回报，来估计价值函数 (value function)。</p>
			</section>
			<section>
				<h2>TD时序差分</h2>
				<p>初访法:\(V(s_1)=\frac{G_{12}+G_{2k}+...}{N(s_1)}\)</p>
				<p>增量式更新:\(Q(s, a) \leftarrow Q(s, a) + \frac{1}{N} \left[ G - Q(s, a) \right]\) </p>
			</section>
			<section>
				<h2>Sarsa与Q-learning</h2>
				<p>Q 学习和 Sarsa 算法的区别在于:区别主要在于它们如何更新Q值:Q-Learning基于最优未来行动更新(即便这个行动并未实际采取 更激进),而Sarsa基于实际采取的下一个行动更新(反映了当前策略 更保守)。</p>
				<img style="border:none" src="img/Sarsa.PNG" width="470px"><img style="border:none;margin-bottom:33px;" src="img/QLearning.PNG" width="470px">
			</section>
			<section>
				<h2>Deep Q Network</h2>
				<p>
					Q学习是一种强大的算法,可以解决简单的强化问题。但是，它无法处理连续状态或连续动作。为了解决前一个问题，可以使用深度学习方法来逼近动作值函数。</p>
				<img src="img/DeepQNetwork.PNG">
				
			</section>
			<section>
				<h2>DQN与Dueling DQN</h2>
				<p>DQN和Dueling DQN的主要区别在于它们如何评估状态。DQN直接输出一个Q值,而Dueling DQN是将输出分为价值函数和动作函数,然后通过它们的和来表示Q值。</p>
				<img src="img/DQN.PNG">
			</section>
         </section>
		 	<section>
			 	<section>
			<h2>3.实现方法</h2>
			<p>
				<p>软硬件要求:OS:Windows 10|Programming language:Python 3.7.4|Framework:OpenCV, Pytorch, Numpy, Gym Browser:Chrome 76</p>
				<p>CPU:Intel Core i5-8300H|RAM	16G|GPU	Nvidia GTX 1060 6G</p>
			</p>
			<a href="#" class="navigate-down">
				<img width="108" height="130" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png"
						alt="Down arrow">
				</a>
			  </section>
			<section>
			<h2>T-rex Runner游戏介绍</h2>
				<p>玩家的目标是控制恐龙，克服尽可能多的障碍。如果恐龙没有失败，游戏的当前分数将随着时间的推移而增加,分数逐渐增加。恐龙在每种状态下都有三个动作可供选择：什么都不做、跳跃或躲避。</p>
			<img src="./img/runnerActions.PNG">  
			</section>
		
			<section data-markdown>
                <script type="text/template">
					## 如何设计奖励函数?
					- 如果一局游戏结束:那么返回的奖励为-1。
					- 如果游戏没有结束:
						- 如果智能体选择跳跃,那么返回的奖励为0。
						- 如果智能体选择其他动作（例如，不做任何事情或者下蹲），那么返回的奖励为0.1。
					
					这种奖励设计的目的是鼓励智能体在不必要跳跃的情况下选择其他动作，因为这样可以获得更高的奖励（0.1比0大）。
                </script>
            </section>
			
			<section>
			<h2>研究方法选型</h2>
				<p>DQN,double DQN,DQN with prioritized experience replay,dueling DQN </p>
				<a>介绍</a>
			</section>
			<section>
			<h2>图像预处理</h2>
				<img src="img/imgPreprocessing.PNG">
			</section>
			<section>
				<h2>卷积神经网络架构</h2>
					<img src="img/CNN.PNG">
			</section>
			<section>
				<h2>Dueling DQN</h2>
					<img src="img/DuelingDQN.PNG">
			</section>
			<section>
				<h2>实验</h2>
					<p>超参数调优</p>
					<p>不同Deep Q网络算法的比较</p>
					<p>批量归一化的影响</p>
			</section>
		</section>
		    <section>
				<section>
			 <h2>4.结果与讨论</h2>
			 <a href="#" class="navigate-down">
				<img width="108" height="130" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png"
					alt="Down arrow">
			 </a>
			</section>
			<section>
				<h2>超参数调优</h2>
				<img src="./img/bat.PNG">	
			</section>
			<section>
				<h2>BatchSize</h2>
				<p>epoch 时三条曲线的平均得分80无处不在800.在这三者中，最稳定的是批量大小128.</p>
				<img src="img/batchSize.PNG">
			</section>
			<section>
				<h2>其他参数</h2>
				<p>探索步骤</p>
				<p>\[\gamma\]</p>
			</section>
			<section>
				<h2>训练结果</h2>
				<p>DQN</p>
				<img src="img/DQNchart.PNG">
			</section>
			<section>
				<h2>训练结果</h2>
				<p>DoubleDQN</p>
				<img src="img/doubleDQNChart.PNG">
			</section>
			<section>
				<h2>训练结果</h2>
				<p>DQN with PRE</p>
				<img src="img/DQNwithPRE.PNG">
			</section>
			<section>
				<h2>批量归一化</h2>
				<img src="img/comparsion.PNG">
			</section>
			<section>
				<h2>进一步讨论</h2>
				<img src="img/furtherDiscussion.PNG">
			</section>
			<section>
				<h2>测试结果</h2>
				<img src="img/conclusion.PNG">
			</section>
			<section>
				<h2>测试结果</h2>
				<img src="img/conclusion2.PNG">
				<p>根据表4.5,无论算法如何，批量归一化都会提高模型的性能，甚至增加了 DQN 和 PER 的均值。然而，很难说BN在决斗DQN中的作用是积极的还是不积极的。从图 4.11 中可以看出，没有 BN 的那个具有更多的异常值，即使其均值较高，也会导致高方差。考虑对异常值数据不敏感的中位数，BN 的中位数更好，最低分数超过 200，这在其他算法中脱颖而出。由于分数 43 表示智能体第一次遇到障碍物，因此很容易推断出所有经过训练的模型除了与 BN 决斗 DQN 外，至少一次都无法跳过第一个仙人掌。但是决斗 DQN 没有完全训练，这可以从图 4.8 的训练结果中看出。这也是高方差的原因之一，正如我们在箱线图中看到的那样。受过决斗DQN训练的特工至少三次超过8000。</p>
			</section>
		</section>
		<section>
			<h2>结论和未来工作</h2>
			<p>
				该项目旨在创建一个由四种算法训练的智能体来玩霸王龙奔跑者，并研究批量归一化在强化学习中的影响。
                前一个目标已经实现，但由于游戏环境问题而优先重播。但是，所有其他算法都已成功实现并取得了很好的效果，尤其是 DQN 和决斗 DQN。他们俩都可以取得比人类专家更好的结果。尽管训练阶段的平均得分不稳定，但批量归一化对本项目中的所有 DQN 算法都显示出相对积极的影响。
                在进一步的研究中，应该首先重新开发游戏环境，以便在神经网络计算时添加暂停函数值或进行优化。之后可以测试优先体验重播。在本项目中，仅实现了基于比例的方法，因此将来也可以研究基于秩的优先级。可以开发进一步的算法组合，例如具有优先体验重放的决斗 DQN。还可以实施基于策略的算法（如 PPO）来训练代理。
                有一个有趣的想法尚未实施。考虑到障碍物的移动速度逐渐增加，我们可以将游戏分为几个阶段。每个阶段都有一个神经网络，该神经网络由前一阶段初始化，并将独立训练。这个想法的直觉是，当智能体在不同阶段运行时，跳跃的结果会发生变化。这也可能是高方差的原因之一，因为当智能体在后期学会了如何获得更好的分数时，它会在早期阶段忘记最佳策略。
            </p>
		</section>

	   <div class="slides">
            <section data-markdown>
                <script type="text/template">
                    ## Hello, Markdown!

                    This is a sample slide written in Markdown.

                    - Point 1
                    - Point 2
                    - Point 3
                </script>
            </section>
        </div>
        
		<section>
			<section style="text-align: left;">
				<h2>感谢老师及同学的聆听</h2>
				<p>
					- <a href="https://slides.com">Try the online editor</a> <br>
					- <a href="https://github.com/hakimel/reveal.js">Source code &amp; documentation</a>
				</p>
			</section>
			<section id="fragments">
				<h2>致谢</h2>
				<p>老师</p>
				<p class="fragment">同学</p>
				<p><span class="fragment ">的</span> <span class="fragment">聆听    </span> <span
						class="fragment">slide.</span></p>

				<aside class="notes">
					This slide has fragments which are also stepped through in the notes window.
				</aside>
			 </section>
				  <section>
					<section id="fragments">
						<h2>Fragments</h2>
						<p>Hit the next arrow...</p>
						<p class="fragment">... to step through ...</p>
						<p><span class="fragment">... a</span> <span class="fragment">fragmented</span> <span class="fragment">slide.</span></p>

						<aside class="notes">
							This slide has fragments which are also stepped through in the notes window.
						</aside>
					</section>
					<section>
						<h2>Fragment Styles</h2>
						<p>There's different types of fragments, like:</p>
						<p class="fragment grow">grow</p>
						<p class="fragment shrink">shrink</p>
						<p class="fragment fade-out">fade-out</p>
						<p>
							<span style="display: inline-block;" class="fragment fade-right">fade-right, </span>
							<span style="display: inline-block;" class="fragment fade-up">up, </span>
							<span style="display: inline-block;" class="fragment fade-down">down, </span>
							<span style="display: inline-block;" class="fragment fade-left">left</span>
						</p>
						<p class="fragment fade-in-then-out">fade-in-then-out</p>
						<p class="fragment fade-in-then-semi-out">fade-in-then-semi-out</p>
						<p>Highlight <span class="fragment highlight-red">red</span> <span class="fragment highlight-blue">blue</span> <span class="fragment highlight-green">green</span></p>
					</section>
				</section>

				<section id="transitions">
					<h2>Transition Styles</h2>
					<p>
						You can select from different transitions, like: <br>
						<a href="?transition=none#/transitions">None</a> -
						<a href="?transition=fade#/transitions">Fade</a> -
						<a href="?transition=slide#/transitions">Slide</a> -
						<a href="?transition=convex#/transitions">Convex</a> -
						<a href="?transition=concave#/transitions">Concave</a> -
						<a href="?transition=zoom#/transitions">Zoom</a>
					</p>
				</section>

				<section id="themes">
					<h2>Themes</h2>
					<p>
						reveal.js comes with a few themes built in: <br>
						<!-- Hacks to swap themes after the page has loaded. Not flexible and only intended for the reveal.js demo deck. -->
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/black.css'); return false;">Black (default)</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/white.css'); return false;">White</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/league.css'); return false;">League</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/sky.css'); return false;">Sky</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/beige.css'); return false;">Beige</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/simple.css'); return false;">Simple</a> <br>
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/serif.css'); return false;">Serif</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/blood.css'); return false;">Blood</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/night.css'); return false;">Night</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/moon.css'); return false;">Moon</a> -
						<a href="#/themes" onclick="document.getElementById('theme').setAttribute('href','dist/theme/solarized.css'); return false;">Solarized</a>
					</p>
				</section>

				<section>
					<section data-background="#dddddd">
						<h2>Slide Backgrounds</h2>
						<p>
							Set <code>data-background="#dddddd"</code> on a slide to change the background color. All CSS color formats are supported.
						</p>
						<a href="#" class="navigate-down">
							<img class="r-frame" style="background: rgba(255,255,255,0.1);" width="178" height="238" data-src="https://static.slid.es/reveal/arrow.png" alt="Down arrow">
						</a>
					</section>
					<section data-background-gradient="linear-gradient(to bottom, #283b95, #17b2c3)">
						<h2>Gradient Backgrounds</h2>
						<pre><code class="hljs html wrap">&lt;section data-background-gradient=
							"linear-gradient(to bottom, #ddd, #191919)"&gt;</code></pre>
					</section>
					<section data-background="https://static.slid.es/reveal/image-placeholder.png">
						<h2>Image Backgrounds</h2>
						<pre><code class="hljs html">&lt;section data-background="image.png"&gt;</code></pre>
					</section>
					<section data-background="https://static.slid.es/reveal/image-placeholder.png" data-background-repeat="repeat" data-background-size="100px">
						<h2>Tiled Backgrounds</h2>
						<pre><code class="hljs html" style="word-wrap: break-word;">&lt;section data-background="image.png" data-background-repeat="repeat" data-background-size="100px"&gt;</code></pre>
					</section>
					<section data-background-video="https://static.slid.es/site/homepage/v1/homepage-video-editor.mp4" data-background-color="#000000">
						<div style="background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px;">
							<h2>Video Backgrounds</h2>
							<pre><code class="hljs html" style="word-wrap: break-word;">&lt;section data-background-video="video.mp4,video.webm"&gt;</code></pre>
						</div>
					</section>
					<section data-background="http://i.giphy.com/90F8aUepslB84.gif">
						<h2>... and GIFs!</h2>
					</section>
				</section>

				<section data-transition="slide" data-background="#4d7e65" data-background-transition="zoom">
					<h2>Background Transitions</h2>
					<p>
						Different background transitions are available via the backgroundTransition option. This one's called "zoom".
					</p>
					<pre><code class="hljs javascript">Reveal.configure({ backgroundTransition: 'zoom' })</code></pre>
				</section>

				<section data-transition="slide" data-background="#b5533c" data-background-transition="zoom">
					<h2>Background Transitions</h2>
					<p>
						You can override background transitions per-slide.
					</p>
					<pre><code class="hljs html" style="word-wrap: break-word;">&lt;section data-background-transition="zoom"&gt;</code></pre>
				</section>

				<section data-background-iframe="https://hakim.se" data-background-interactive>
					<div style="position: absolute; width: 40%; right: 0; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
						<h2>Iframe Backgrounds</h2>
						<p>Since reveal.js runs on the web, you can easily embed other web content. Try interacting with the page in the background.</p>
					</div>
				</section>

				<section>
					<h2>Marvelous List</h2>
					<ul>
						<li>No order here</li>
						<li>Or here</li>
						<li>Or here</li>
						<li>Or here</li>
					</ul>
				</section>

				<section>
					<h2>Fantastic Ordered List</h2>
					<ol>
						<li>One is smaller than...</li>
						<li>Two is smaller than...</li>
						<li>Three!</li>
					</ol>
				</section>

				<section>
					<h2>Tabular Tables</h2>
					<table>
						<thead>
							<tr>
								<th>Item</th>
								<th>Value</th>
								<th>Quantity</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>Apples</td>
								<td>$1</td>
								<td>7</td>
							</tr>
							<tr>
								<td>Lemonade</td>
								<td>$2</td>
								<td>18</td>
							</tr>
							<tr>
								<td>Bread</td>
								<td>$3</td>
								<td>2</td>
							</tr>
						</tbody>
					</table>
				</section>

				<section>
					<h2>Clever Quotes</h2>
					<p>
						These guys come in two forms, inline: <q cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">The nice thing about standards is that there are so many to choose from</q> and block:
					</p>
					<blockquote cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
						&ldquo;For years there has been a theory that millions of monkeys typing at random on millions of typewriters would
						reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.&rdquo;
					</blockquote>
				</section>

				<section>
					<h2>Intergalactic Interconnections</h2>
					<p>
						You can link between slides internally,
						<a href="#/2/3">like this</a>.
					</p>
				</section>

				<section>
					<h2>Speaker View</h2>
					<p>There's a <a href="https://revealjs.com/speaker-view/">speaker view</a>. It includes a timer, preview of the upcoming slide as well as your speaker notes.</p>
					<p>Press the <em>S</em> key to try it out.</p>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>
				</section>

				<section>
					<h2>Export to PDF</h2>
					<p>Presentations can be <a href="https://revealjs.com/pdf-export/">exported to PDF</a>, here's an example:</p>
					<iframe data-src="https://www.slideshare.net/slideshow/embed_code/42840540" width="445" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:3px solid #666; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
				</section>

				<section>
					<h2>Global State</h2>
					<p>
						Set <code>data-state="something"</code> on a slide and <code>"something"</code>
						will be added as a class to the document element when the slide is open. This lets you
						apply broader style changes, like switching the page background.
					</p>
				</section>

				<section data-state="customevent">
					<h2>State Events</h2>
					<p>
						Additionally custom events can be triggered on a per slide basis by binding to the <code>data-state</code> name.
					</p>
					<pre><code class="javascript" data-trim contenteditable style="font-size: 18px;">
Reveal.on( 'customevent', function() {
	console.log( '"customevent" has fired' );
} );
					</code></pre>
				</section>

				<section>
					<h2>Take a Moment</h2>
					<p>
						Press B or . on your keyboard to pause the presentation. This is helpful when you're on stage and want to take distracting slides off the screen.
					</p>
				</section>

				<section>
					<h2>Much more</h2>
					<ul>
						<li>Right-to-left support</li>
						<li><a href="https://revealjs.com/api/">Extensive JavaScript API</a></li>
						<li><a href="https://revealjs.com/auto-slide/">Auto-progression</a></li>
						<li><a href="https://revealjs.com/backgrounds/#parallax-background">Parallax backgrounds</a></li>
						<li><a href="https://revealjs.com/keyboard/">Custom keyboard bindings</a></li>
					</ul>
				</section>

				<section style="text-align: left;">
					<h1>THE END</h1>
					<p>
						- <a href="https://slides.com">Try the online editor</a> <br>
						- <a href="https://github.com/hakimel/reveal.js">Source code &amp; documentation</a>
					</p>
				</section>

			</div>

		</div>

		<!-- <script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script> -->
		<script src='https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/js/reveal.min.js'></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/lib/js/head.min.js'></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/plugin/markdown/marked.min.js" ></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/plugin/markdown/markdown.min.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="dist/reveal.js"></script>
<script src="plugin/zoom/zoom.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				hash: true,
				
				transition: "convex",
				// Learn about plugins: https://revealjs.com/plugins/
				//plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight ]

				plugins: [RevealZoom,RevealNotes,RevealMarkdown],
	            dependencies:[
				{ src:"https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.4.1/plugin/math/math.min.js", async: true } // MathJax
				]
			});

		</script>

	</body>
</html>
